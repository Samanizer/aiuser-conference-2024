{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh1QlZQ3wVDP"
      },
      "outputs": [],
      "source": [
        "# Installing\n",
        "!pip install -q openai langchain gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key\")"
      ],
      "metadata": {
        "id": "PgSJKv7MwimK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize chat model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        ")"
      ],
      "metadata": {
        "id": "AfebTzfaUwni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a prompt template\n",
        "template = \"\"\"You are an expert travel consultant. You give specific and thoughtful advice to potential travelers.\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", template),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "fcAkCPMLU5fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Create conversation history store\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)"
      ],
      "metadata": {
        "id": "LGK1nFeXVBZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "\n",
        "# Compose components in a chain using LCEL\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
        "    )\n",
        "    | chat_prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "74fgeyisVMfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response function for ChatInterface\n",
        "def stream_response(input, _):\n",
        "    if input is not None:\n",
        "        partial_message = \"\"\n",
        "        for chunk in chain.stream({\"input\": input}): # LCEL streaming syntax\n",
        "            partial_message += chunk.content\n",
        "            yield partial_message"
      ],
      "metadata": {
        "id": "tcaSL5L-VR-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(\n",
        "    stream_response, retry_btn=False, undo_btn=False, clear_btn=False\n",
        ").queue().launch()"
      ],
      "metadata": {
        "id": "vhbBUF9IVdKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt chaining example\n",
        "\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# chain 1: Generating premises\n",
        "comedian = \"Kevin Hart\"\n",
        "\n",
        "template = \"\"\"In bullet point form, wrie 5-10 potential topics for jokes about going to the movies that might be employed by {comedian}. Don't write the entire jokes just the premises. Mention the name of the comedian at the start of the bullet point\\n\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"comedian\"], template=template)\n",
        "premises_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# chain 2: Generate storyboard\n",
        "template = \"\"\"Here are a list of premises for jokes in the style of our requested comedian:\n",
        "  {premises}\n",
        "  From these bullet points, select 2-3 premises that could be combined into a funny joke, and note how they might flow with one and other.\\n\\n\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"premises\"],template=template)\n",
        "storyboard_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Chain 3: Create joke\n",
        "template = \"\"\"Using the following storyboard:\n",
        "{storyboard}\n",
        "  Please write a comedic joke in the style of our requested comedian.\n",
        "  \"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"storyboard\"],template=template)\n",
        "final_joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "final_joke_chain_seq = SimpleSequentialChain(\n",
        "      chains=[premises_chain, storyboard_chain, final_joke_chain],verbose=True\n",
        ")\n",
        "\n",
        "print(final_joke_chain_seq.run(comedian))"
      ],
      "metadata": {
        "id": "-GWpNUO2CCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Final exercise:\n",
        "\n",
        "1) Like with our travel consultant above, we would like to create a chatbot.\n",
        "\n",
        "Rather than a travel consultant, we would like this chatbot to be a financial consultant of which the user can ask personal finance questions.\n",
        "\n",
        "Also, we would like to create a Gradio interface with streamed responses.\n",
        "\n",
        "\n",
        "2) Create a prompt chain to write a poem about McDonald's in the style of a user supplied poet (similar to the comedian joke chain we created above).\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kxNmL2yUD7FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbOiuDoLuwaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}